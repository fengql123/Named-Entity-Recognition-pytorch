{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed00e960",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import pipeline, AutoTokenizer, DataCollatorForTokenClassification, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "from torchtext.vocab import build_vocab_from_iterator, GloVe\n",
    "from app.utils import getData, iterator, train_and_eval, predict, Data\n",
    "from app.model import NER, NER_transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ff4e42",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f490c35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/ner_datasetreference.csv\", encoding='latin1')\n",
    "sentences, labels = getData(data)\n",
    "split_idx = int(0.9 * len(sentences))\n",
    "train, train_labels, test, test_labels = sentences[:split_idx], labels[:split_idx], sentences[split_idx:], labels[split_idx:]\n",
    "vocab = build_vocab_from_iterator(iterator(train), specials=[\"<unk>\", \"<pad>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "labels_dict = build_vocab_from_iterator(iterator(train_labels), specials=[\"<pad>\"])\n",
    "glove_vocab = GloVe(name='42B', dim=300)\n",
    "use_glove = False\n",
    "max_len = max([len(s) for s in sentences])\n",
    "train_pipeline = Data(train, train_labels, max_len, vocab, glove_vocab, labels_dict, use_glove=use_glove)\n",
    "test_pipeline = Data(test, test_labels, max_len, vocab, glove_vocab, labels_dict, use_glove=use_glove)\n",
    "train_loader = DataLoader(train_pipeline, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f21eda4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104\n"
     ]
    }
   ],
   "source": [
    "print(max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a1ce58",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "077ebd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "tag_size = len(labels_dict)\n",
    "embed_size = 300\n",
    "num_layers = 3\n",
    "hidden_size = 64\n",
    "n_epochs = 20\n",
    "learning_rate = 1e-3\n",
    "n_head = 5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f8cb45",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a1c629e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = NER(vocab_size, embed_size, hidden_size, num_layers, tag_size, use_glove=use_glove).to(device=device)\n",
    "model2 = NER_transformer(vocab_size, embed_size, num_layers, tag_size, n_head, use_glove=use_glove).to(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca4ee95",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32cabe69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss of epoch 1 is 0.63421\n",
      "test acc after epoch 1 is 0.92500\n",
      "training loss of epoch 2 is 0.58966\n",
      "test acc after epoch 2 is 0.93448\n",
      "training loss of epoch 3 is 0.58042\n",
      "test acc after epoch 3 is 0.94019\n",
      "training loss of epoch 4 is 0.57586\n",
      "test acc after epoch 4 is 0.94348\n",
      "training loss of epoch 5 is 0.57162\n",
      "test acc after epoch 5 is 0.94448\n",
      "training loss of epoch 6 is 0.57040\n",
      "test acc after epoch 6 is 0.94524\n",
      "training loss of epoch 7 is 0.56931\n",
      "test acc after epoch 7 is 0.94630\n",
      "training loss of epoch 8 is 0.56762\n",
      "test acc after epoch 8 is 0.94716\n",
      "training loss of epoch 9 is 0.56699\n",
      "test acc after epoch 9 is 0.94846\n",
      "training loss of epoch 10 is 0.56618\n",
      "test acc after epoch 10 is 0.94791\n",
      "training loss of epoch 11 is 0.56488\n",
      "test acc after epoch 11 is 0.94764\n",
      "training loss of epoch 12 is 0.56379\n",
      "test acc after epoch 12 is 0.95008\n",
      "training loss of epoch 13 is 0.56352\n",
      "test acc after epoch 13 is 0.94977\n",
      "training loss of epoch 14 is 0.56319\n",
      "test acc after epoch 14 is 0.95001\n",
      "training loss of epoch 15 is 0.56178\n",
      "test acc after epoch 15 is 0.94919\n",
      "training loss of epoch 16 is 0.56157\n",
      "test acc after epoch 16 is 0.95144\n",
      "training loss of epoch 17 is 0.56162\n",
      "test acc after epoch 17 is 0.95102\n",
      "training loss of epoch 18 is 0.56047\n",
      "test acc after epoch 18 is 0.95223\n",
      "training loss of epoch 19 is 0.56033\n",
      "test acc after epoch 19 is 0.94981\n",
      "training loss of epoch 20 is 0.56220\n",
      "test acc after epoch 20 is 0.95123\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model2.parameters(), lr=learning_rate)\n",
    "for epoch in range(n_epochs):\n",
    "    train_and_eval(model2, train_loader, test_loader, optimizer, criterion, device, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0a51fb",
   "metadata": {},
   "source": [
    "### Fine Tuning Using DistillBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7decb80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset wnut_17 (C:\\Users\\fengq\\.cache\\huggingface\\datasets\\wnut_17\\wnut_17\\1.0.0\\077c7f08b8dbc800692e8c9186cdf3606d5849ab0e7be662e6135bb10eba54f9)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39224c6f17ab4c87b66258c1d513cb98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\fengq\\.cache\\huggingface\\datasets\\wnut_17\\wnut_17\\1.0.0\\077c7f08b8dbc800692e8c9186cdf3606d5849ab0e7be662e6135bb10eba54f9\\cache-c2f22eacb5353639.arrow\n",
      "Loading cached processed dataset at C:\\Users\\fengq\\.cache\\huggingface\\datasets\\wnut_17\\wnut_17\\1.0.0\\077c7f08b8dbc800692e8c9186cdf3606d5849ab0e7be662e6135bb10eba54f9\\cache-d00931926d0d0eb0.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66be7e8467db4ea7aca332823de5d723",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForTokenClassification: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: tokens, id, ner_tags. If tokens, id, ner_tags are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "C:\\Users\\fengq\\miniconda3\\envs\\torch\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3394\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 639\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='639' max='639' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [639/639 01:06, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.270019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.255120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.188900</td>\n",
       "      <td>0.265892</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: tokens, id, ner_tags. If tokens, id, ner_tags are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1287\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: tokens, id, ner_tags. If tokens, id, ner_tags are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1287\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results\\checkpoint-500\n",
      "Configuration saved in ./results\\checkpoint-500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-500\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-500\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: tokens, id, ner_tags. If tokens, id, ner_tags are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1287\n",
      "  Batch size = 16\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=639, training_loss=0.1645464277789813, metrics={'train_runtime': 68.1453, 'train_samples_per_second': 149.416, 'train_steps_per_second': 9.377, 'total_flos': 137822474941512.0, 'train_loss': 0.1645464277789813, 'epoch': 3.0})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "wnut = load_dataset(\"wnut_17\")\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "tokenized_wnut = wnut.map(tokenize_and_align_labels, batched=True)\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=14)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_wnut[\"train\"],\n",
    "    eval_dataset=tokenized_wnut[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b63c6ac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-person', 'O', 'O', 'B-location', 'O', 'O', 'O', 'B-location']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline(\"ner\", model=model, tokenizer=tokenizer, device=0)\n",
    "label_list = wnut[\"train\"].features[f\"ner_tags\"].feature.names\n",
    "text = \"Isaiah expected the China to return to Taiwan\"\n",
    "def predict(classifier, text, label_list):\n",
    "    result = []\n",
    "    for x in classifier(text):\n",
    "        x = x['entity'].split(\"_\")\n",
    "        x = int(x[1])\n",
    "        result.append(label_list[x])\n",
    "    return result\n",
    "predict(classifier, text, label_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089cbd5f",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4bf60b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-per', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'B-geo']\n"
     ]
    }
   ],
   "source": [
    "print(predict(model2, \"Isaiah expected the China to return to Taiwan\", max_len, vocab, glove_vocab, labels_dict, use_glove, device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bca841",
   "metadata": {},
   "source": [
    "### Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8a59312",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model2.state_dict(), './app/saved_no_glove_transformer.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
